{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2983d3b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Country_Region'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m country_info \u001b[38;5;241m=\u001b[39m country_info[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry_Region\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     66\u001b[0m country_info \u001b[38;5;241m=\u001b[39m country_info\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry_Region\u001b[39m\u001b[38;5;124m'\u001b[39m, keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m doc_final_country \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_confirmed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCountry_Region\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m doc_final_country \u001b[38;5;241m=\u001b[39m doc_final_country\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     70\u001b[0m doc_final_country[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m doc_final_country[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(create_flag_link)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:107\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 107\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:700\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross \u001b[38;5;241m=\u001b[39m cross_col\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# note this function has side effects\u001b[39;00m\n\u001b[0;32m    696\u001b[0m (\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m--> 700\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1110\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     right_keys\u001b[38;5;241m.\u001b[39mappend(rk)\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1110\u001b[0m     left_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1111\u001b[0m     join_names\u001b[38;5;241m.\u001b[39mappend(lk)\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:1840\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1838\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Country_Region'"
     ]
    }
   ],
   "source": [
    "#1. 가장 최근 dataset적용 코로나 확진자수 시각화 \n",
    "import pandas as pd\n",
    "import json, os\n",
    "years=['2020','2021','2022']\n",
    "i=0 \n",
    "while i < 3: \n",
    "    \n",
    "    with open('COVID-19-master/csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    def country_name_convert(row):\n",
    "        if row['Country_Region'] in json_data:\n",
    "            return json_data[row['Country_Region']]\n",
    "        return row['Country_Region']\n",
    "    def create_dateframe(filename):\n",
    "        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n",
    "        try:\n",
    "            doc = doc[['Country_Region', 'Confirmed']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "        except:\n",
    "            doc = doc[['Country/Region', 'Confirmed']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "            doc.columns = ['Country_Region', 'Confirmed']\n",
    "        doc = doc.dropna(subset=['Confirmed']) # 3. 특정 컬럼에 없는 데이터 삭제하기\n",
    "        doc['Country_Region'] = doc.apply(country_name_convert, axis=1) # 4. 'Country_Region'의 국가명을 여러 파일에 일관되doc = doc.astype({'Confirmed': 'int64'}) # 5. 특정 컬럼의 데이터 타입 변경하기\n",
    "        doc = doc.groupby('Country_Region').sum() # 6. 특정 컬럼으로 중복된 데이터를 합치기\n",
    "        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n",
    "        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n",
    "        doc.columns = [date_column]\n",
    "        return doc\n",
    "\n",
    "    def generate_dateframe_by_path(PATH):\n",
    "    \n",
    "        #years=['2020','2021','2022']\n",
    "            \n",
    "        file_list, csv_list = os.listdir(PATH), list()\n",
    "        first_doc = True\n",
    "        for file in file_list:\n",
    "            if file.split(\"-\")[-1] == years[i]+'.csv':\n",
    "                csv_list.append(file)\n",
    "        csv_list.sort()\n",
    "        for file in csv_list:\n",
    "            doc = create_dateframe(file)\n",
    "            if first_doc:\n",
    "                final_doc, first_doc = doc, False\n",
    "            else:\n",
    "                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n",
    "        final_doc = final_doc.fillna(0)\n",
    "        return final_doc\n",
    "    def create_flag_link(row):\n",
    "        flag_link = 'https://flagcdn.com/48x36/' + row + '.png'\n",
    "        return flag_link\n",
    "\n",
    "\n",
    "\n",
    "    PATH = 'COVID-19-master/csse_covid_19_data/csse_covid_19_daily_report/'\n",
    "    df_confirmed = generate_dateframe_by_path(PATH)\n",
    "    df_confirmed = df_confirmed.astype('int64')\n",
    "\n",
    "    country_info = pd.read_csv(\"COVID-19-master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\", encoding='utf-8-sig',keep_default_na=False)\n",
    "    country_info = country_info[['iso2', 'Country_Region']]\n",
    "    country_info = country_info.drop_duplicates(subset='Country_Region', keep='last')\n",
    "                           \n",
    "    doc_final_country = pd.merge(df_confirmed, country_info, how='left', on='Country_Region')\n",
    "    doc_final_country = doc_final_country.dropna(subset=['iso2'])\n",
    "    doc_final_country['iso2'] = doc_final_country['iso2'].apply(create_flag_link)\n",
    "                           \n",
    "    cols = doc_final_country.columns.tolist()\n",
    "    cols.remove('iso2')\n",
    "    cols.insert(1, 'iso2')\n",
    "    doc_final_country = doc_final_country[cols]\n",
    "    cols[1] = 'Country_Flag'\n",
    "    doc_final_country.columns = cols\n",
    "    doc_final_country['Country_Flag'] = doc_final_country['Country_Flag'].str.lower()\n",
    "                         \n",
    "    doc_final_country.to_csv(\"COVID-19-master/final_covid_data_for_graph\"+years[i]+\".csv\")\n",
    "        \n",
    "    i+=1\n",
    "\n",
    "a = pd.read_csv(\"COVID-19-master/final_covid_data_for_graph2020.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "b = pd.read_csv(\"COVID-19-master/final_covid_data_for_graph2021.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "c = pd.read_csv(\"COVID-19-master/final_covid_data_for_graph2022.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "\n",
    "d = pd.merge(a,b,on ='Country_Region')\n",
    "d = pd.merge(d,c,on ='Country_Region')\n",
    "\n",
    "cols = d.columns.tolist()\n",
    "cols.remove('Country_Flag_y')\n",
    "cols.remove('Country_Flag')\n",
    "d = d[cols]\n",
    "cols[1] = 'Country_Flag'\n",
    "d.columns = cols\n",
    "\n",
    "d.to_csv(\"COVID-19-master/final_covid_data_for_graph 2020~2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efab23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 가장 최근 dataset적용 코로나 사망자 시각화\n",
    "import pandas as pd\n",
    "import json, os\n",
    "years=['2020','2021','2022']\n",
    "i=0 \n",
    "while i < 3: \n",
    "    \n",
    "    with open('COVID-19-master/csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    def country_name_convert(row):\n",
    "        if row['Country_Region'] in json_data:\n",
    "            return json_data[row['Country_Region']]\n",
    "        return row['Country_Region']\n",
    "    def create_dateframe(filename):\n",
    "        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n",
    "        try:\n",
    "            doc = doc[['Country_Region', 'Deaths']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "        except:\n",
    "            doc = doc[['Country/Region', 'Deaths']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "            doc.columns = ['Country_Region', 'Deaths']\n",
    "        doc = doc.dropna(subset=['Deaths']) # 3. 특정 컬럼에 없는 데이터 삭제하기\n",
    "        doc['Country_Region'] = doc.apply(country_name_convert, axis=1) # 4. 'Country_Region'의 국가명을 여러 파일에 일관되doc = doc.astype({'Confirmed': 'int64'}) # 5. 특정 컬럼의 데이터 타입 변경하기\n",
    "        doc = doc.groupby('Country_Region').sum() # 6. 특정 컬럼으로 중복된 데이터를 합치기\n",
    "        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n",
    "        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n",
    "        doc.columns = [date_column]\n",
    "        return doc\n",
    "\n",
    "    def generate_dateframe_by_path(PATH):\n",
    "    \n",
    "        #years=['2020','2021','2022']\n",
    "            \n",
    "        file_list, csv_list = os.listdir(PATH), list()\n",
    "        first_doc = True\n",
    "        for file in file_list:\n",
    "            if file.split(\"-\")[-1] == years[i]+'.csv':\n",
    "                csv_list.append(file)\n",
    "        csv_list.sort()\n",
    "        for file in csv_list:\n",
    "            doc = create_dateframe(file)\n",
    "            if first_doc:\n",
    "                final_doc, first_doc = doc, False\n",
    "            else:\n",
    "                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n",
    "        final_doc = final_doc.fillna(0)\n",
    "        return final_doc\n",
    "    def create_flag_link(row):\n",
    "        flag_link = 'https://flagcdn.com/48x36/' + row + '.png'\n",
    "        return flag_link\n",
    "\n",
    "\n",
    "\n",
    "    PATH = 'COVID-19-master/csse_covid_19_data/csse_covid_19_daily_report/'\n",
    "    df_confirmed = generate_dateframe_by_path(PATH)\n",
    "    df_confirmed = df_confirmed.astype('int64')\n",
    "\n",
    "    country_info = pd.read_csv(\"COVID-19-master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\", encoding='utf-8-sig',keep_default_na=False)\n",
    "    country_info = country_info[['iso2', 'Country_Region']]\n",
    "    country_info = country_info.drop_duplicates(subset='Country_Region', keep='last')\n",
    "                           \n",
    "    doc_final_country = pd.merge(df_confirmed, country_info, how='left', on='Country_Region')\n",
    "    doc_final_country = doc_final_country.dropna(subset=['iso2'])\n",
    "    doc_final_country['iso2'] = doc_final_country['iso2'].apply(create_flag_link)\n",
    "                           \n",
    "    cols = doc_final_country.columns.tolist()\n",
    "    cols.remove('iso2')\n",
    "    cols.insert(1, 'iso2')\n",
    "    doc_final_country = doc_final_country[cols]\n",
    "    cols[1] = 'Country_Flag'\n",
    "    doc_final_country.columns = cols\n",
    "    doc_final_country['Country_Flag'] = doc_final_country['Country_Flag'].str.lower()\n",
    "                         \n",
    "    doc_final_country.to_csv(\"COVID-19-master/final_covid_data_for_Deaths_graph\"+years[i]+\".csv\")\n",
    "        \n",
    "    i+=1\n",
    "\n",
    "a = pd.read_csv(\"COVID-19-master/final_covid_data_for_Deaths_graph2020.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "b = pd.read_csv(\"COVID-19-master/final_covid_data_for_Deaths_graph2021.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "c = pd.read_csv(\"COVID-19-master/final_covid_data_for_Deaths_graph2022.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "\n",
    "d = pd.merge(a,b,on ='Country_Region')\n",
    "d = pd.merge(d,c,on ='Country_Region')\n",
    "\n",
    "cols = d.columns.tolist()\n",
    "cols.remove('Country_Flag_y')\n",
    "cols.remove('Country_Flag')\n",
    "d = d[cols]\n",
    "cols[1] = 'Country_Flag'\n",
    "d.columns = cols\n",
    "\n",
    "d.to_csv(\"COVID-19-master/final_covid_data_for_Deaths_graph 2020~2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5dd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 가장 최근 dataset적용 코로나 회복자 시각화\n",
    "import pandas as pd\n",
    "import json, os\n",
    "years=['2020','2021','2022']\n",
    "i=0 \n",
    "while i < 2: \n",
    "    \n",
    "    with open('COVID-19-master/csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    def country_name_convert(row):\n",
    "        if row['Country_Region'] in json_data:\n",
    "            return json_data[row['Country_Region']]\n",
    "        return row['Country_Region']\n",
    "    def create_dateframe(filename):\n",
    "        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n",
    "        try:\n",
    "            doc = doc[['Country_Region', 'Recovered']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "        except:\n",
    "            doc = doc[['Country/Region', 'Recovered']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "            doc.columns = ['Country_Region', 'Recovered']\n",
    "        doc = doc.dropna(subset=['Recovered']) # 3. 특정 컬럼에 없는 데이터 삭제하기\n",
    "        doc['Country_Region'] = doc.apply(country_name_convert, axis=1) # 4. 'Country_Region'의 국가명을 여러 파일에 일관되doc = doc.astype({'Confirmed': 'int64'}) # 5. 특정 컬럼의 데이터 타입 변경하기\n",
    "        doc = doc.groupby('Country_Region').sum() # 6. 특정 컬럼으로 중복된 데이터를 합치기\n",
    "        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n",
    "        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n",
    "        doc.columns = [date_column]\n",
    "        return doc\n",
    "\n",
    "    def generate_dateframe_by_path(PATH):\n",
    "    \n",
    "        #years=['2020','2021','2022']\n",
    "            \n",
    "        file_list, csv_list = os.listdir(PATH), list()\n",
    "        first_doc = True\n",
    "        for file in file_list:\n",
    "            if file.split(\"-\")[-1] == years[i]+'.csv':\n",
    "                csv_list.append(file)\n",
    "        csv_list.sort()\n",
    "        for file in csv_list:\n",
    "            doc = create_dateframe(file)\n",
    "            if first_doc:\n",
    "                final_doc, first_doc = doc, False\n",
    "            else:\n",
    "                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n",
    "        final_doc = final_doc.fillna(0)\n",
    "        return final_doc\n",
    "    def create_flag_link(row):\n",
    "        flag_link = 'https://flagcdn.com/48x36/' + row + '.png'\n",
    "        return flag_link\n",
    "\n",
    "\n",
    "\n",
    "    PATH = 'COVID-19-master/csse_covid_19_data/csse_covid_19_daily_report/'\n",
    "    df_confirmed = generate_dateframe_by_path(PATH)\n",
    "    df_confirmed = df_confirmed.astype('int64')\n",
    "\n",
    "    country_info = pd.read_csv(\"COVID-19-master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\", encoding='utf-8-sig',keep_default_na=False, na_values='NaN')\n",
    "    country_info = country_info[['iso2', 'Country_Region']]\n",
    "    country_info = country_info.drop_duplicates(subset='Country_Region', keep='last')\n",
    "                           \n",
    "    doc_final_country = pd.merge(df_confirmed, country_info, how='left', on='Country_Region')\n",
    "    doc_final_country = doc_final_country.dropna(subset=['iso2'])\n",
    "    doc_final_country['iso2'] = doc_final_country['iso2'].apply(create_flag_link)\n",
    "                           \n",
    "    cols = doc_final_country.columns.tolist()\n",
    "    cols.remove('iso2')\n",
    "    cols.insert(1, 'iso2')\n",
    "    doc_final_country = doc_final_country[cols]\n",
    "    cols[1] = 'Country_Flag'\n",
    "    doc_final_country.columns = cols\n",
    "    doc_final_country['Country_Flag'] = doc_final_country['Country_Flag'].str.lower()\n",
    "                         \n",
    "    doc_final_country.to_csv(\"COVID-19-master/final_covid_data_for_Recovered_graph\"+years[i]+\".csv\")\n",
    "        \n",
    "    i+=1\n",
    "\n",
    "a = pd.read_csv(\"COVID-19-master/final_covid_data_for_Recovered_graph2020.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "b = pd.read_csv(\"COVID-19-master/final_covid_data_for_Recovered_graph2021.csv\", encoding = 'utf-8-sig', index_col=0)\n",
    "\n",
    "\n",
    "d = pd.merge(a,b,on ='Country_Region')\n",
    "\n",
    "cols = d.columns.tolist()\n",
    "cols.remove('Country_Flag_y')\n",
    "\n",
    "d = d[cols]\n",
    "cols[1] = 'Country_Flag'\n",
    "d.columns = cols\n",
    "\n",
    "d.to_csv(\"COVID-19-master/final_covid_data_for_Recovered_graph 2020~2021.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9bfe4",
   "metadata": {},
   "source": [
    " 1. 가장 최근 dataset 적용 미국내 주별 코로나 확진자수 시각화>\n",
    " Dummy Bonus Points 중국내 도시별 확진자수 시각화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e680015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "years=['2020','2021','2022']\n",
    "i=0 \n",
    "while i < 3:\n",
    "    with open('COVID-19-master/csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    def country_name_convert(row):\n",
    "        if row['Country_Region'] in json_data:\n",
    "            return json_data[row['Country_Region']]\n",
    "        return row['Country_Region']\n",
    "\n",
    "    def create_dateframe(filename):\n",
    "        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n",
    "\n",
    "\n",
    "        try:\n",
    "            doc=doc[['Province_State', 'Country_Region', 'Confirmed']]\n",
    "        except:\n",
    "            doc = doc[['Province/State', 'Country/Region', 'Confirmed']] # 1. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "            doc.columns = ['Province_State', 'Country_Region', 'Confirmed']\n",
    "        doc['Country_Region'] = doc.apply(country_name_convert, axis=1)\n",
    "        doc = doc[doc['Country_Region'] == 'US']\n",
    "        doc = doc.dropna(subset=['Confirmed']) # 2. 특정 컬럼에 없는 데이터 삭제하기\n",
    "        doc = doc.astype({'Confirmed': 'int64'}) # 3. 특정 컬럼의 데이터 타입 변경하기    \n",
    "        doc.Province_State=doc.Province_State.apply(lambda x : x.split(',')[0])\n",
    "        doc = doc.groupby('Province_State').sum()\n",
    "\n",
    "        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n",
    "        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n",
    "        doc.columns = [date_column]\n",
    "        return doc\n",
    "    import os\n",
    "    \n",
    "    def generate_dateframe_by_path(PATH):\n",
    "        file_list, csv_list = os.listdir(PATH), list()\n",
    "        first_doc = True\n",
    "        for file in file_list:\n",
    "            if file.split(\"-\")[-1] == years[i]+'.csv':\n",
    "                csv_list.append(file)\n",
    "        csv_list.sort()\n",
    "   \n",
    "        for file in csv_list:\n",
    "            doc = create_dateframe(file)\n",
    "            if first_doc:\n",
    "                final_doc, first_doc = doc, False\n",
    "            else:\n",
    "                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n",
    "        final_doc = final_doc.fillna(0)\n",
    "        return final_doc\n",
    "   \n",
    "    PATH = 'COVID-19-master/csse_covid_19_data/csse_covid_19_daily_report/'\n",
    "    doc = generate_dateframe_by_path(PATH)\n",
    "    doc = doc.astype('int64')\n",
    "    doc.to_csv(\"COVID-19-master/final_df USA\"+years[i]+\".csv\")\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "a = pd.read_csv(\"COVID-19-master/final_df USA2020.csv\", encoding = 'utf-8-sig')\n",
    "b = pd.read_csv(\"COVID-19-master/final_df USA2021.csv\", encoding = 'utf-8-sig')\n",
    "c = pd.read_csv(\"COVID-19-master/final_df USA2022.csv\", encoding = 'utf-8-sig')\n",
    "\n",
    "d = pd.merge(a,b,on ='Province_State')\n",
    "d = pd.merge(d,c,on ='Province_State')\n",
    "d.to_csv(\"COVID-19-master/final_covid_data_for_graph USA 2020~2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "388fe6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "years=['2020','2021','2022']\n",
    "i=0 \n",
    "while i < 3:\n",
    "    with open('COVID-19-master/csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    def country_name_convert(row):\n",
    "        if row['Country_Region'] in json_data:\n",
    "            return json_data[row['Country_Region']]\n",
    "        return row['Country_Region']\n",
    "\n",
    "    def create_dateframe(filename):\n",
    "        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n",
    "\n",
    "\n",
    "        try:\n",
    "            doc=doc[['Province_State', 'Country_Region', 'Confirmed']]\n",
    "        except:\n",
    "            doc = doc[['Province/State', 'Country/Region', 'Confirmed']] # 1. 특정 컬럼만 선택해서 데이터프레임 만들기\n",
    "            doc.columns = ['Province_State', 'Country_Region', 'Confirmed']\n",
    "        \n",
    "        doc['Country_Region'] = doc.apply(country_name_convert, axis=1)\n",
    "        doc = doc[doc['Country_Region'] == 'China']\n",
    "        doc = doc.dropna(subset=['Confirmed']) # 2. 특정 컬럼에 없는 데이터 삭제하기\n",
    "        doc = doc.astype({'Confirmed': 'int64'}) # 3. 특정 컬럼의 데이터 타입 변경하기    \n",
    "        #doc.Province_State=doc.Province_State.apply(lambda x : x.split(',')[0])\n",
    "        doc = doc.groupby('Province_State').sum()\n",
    "\n",
    "        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n",
    "        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n",
    "        doc.columns = [date_column]\n",
    "        return doc\n",
    "    import os\n",
    "    \n",
    "    def generate_dateframe_by_path(PATH):\n",
    "        file_list, csv_list = os.listdir(PATH), list()\n",
    "        first_doc = True\n",
    "        for file in file_list:\n",
    "            if file.split(\"-\")[-1] == years[i]+'.csv':\n",
    "                csv_list.append(file)\n",
    "        csv_list.sort()\n",
    "   \n",
    "        for file in csv_list:\n",
    "            doc = create_dateframe(file)\n",
    "            if first_doc:\n",
    "                final_doc, first_doc = doc, False\n",
    "            else:\n",
    "                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n",
    "        final_doc = final_doc.fillna(0)\n",
    "        return final_doc\n",
    "   \n",
    "    PATH = 'COVID-19-master/csse_covid_19_data/csse_covid_19_daily_report/'\n",
    "    doc = generate_dateframe_by_path(PATH)\n",
    "    doc = doc.astype('int64')\n",
    "    doc.to_csv(\"COVID-19-master/final_df China\"+years[i]+\".csv\")\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "a = pd.read_csv(\"COVID-19-master/final_df China2020.csv\", encoding = 'utf-8-sig')\n",
    "b = pd.read_csv(\"COVID-19-master/final_df China2021.csv\", encoding = 'utf-8-sig')\n",
    "c = pd.read_csv(\"COVID-19-master/final_df China2022.csv\", encoding = 'utf-8-sig')\n",
    "\n",
    "d = pd.merge(a,b,on ='Province_State')\n",
    "d = pd.merge(d,c,on ='Province_State')\n",
    "d.to_csv(\"COVID-19-master/final_covid_data_for_graph China 2020~2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09acc96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbde04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
